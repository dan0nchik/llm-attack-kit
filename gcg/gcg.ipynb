{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlZnQF8aw_bH"
      },
      "outputs": [],
      "source": [
        "!pip install nanogcg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "HF_TOKEN=userdata.get('HF_TOKEN')\n",
        "\n",
        "if HF_TOKEN:\n",
        "    login(HF_TOKEN)\n",
        "    print(\"Successfully logged in to Hugging Face!\")\n",
        "else:\n",
        "    print(\"Token is not set. Please save the token first.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcQSkCYQyt_U",
        "outputId": "e2d75c7d-2218-4d2c-f53b-6a85e80cd0c2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully logged in to Hugging Face!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "splits = {'harmful': 'data/harmful-behaviors.csv', 'benign': 'data/benign-behaviors.csv'}\n",
        "df = pd.read_csv(\"hf://datasets/JailbreakBench/JBB-Behaviors/\" + splits[\"harmful\"])"
      ],
      "metadata": {
        "id": "gCR1dlkmBVp9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answers = []\n",
        "prompts = []"
      ],
      "metadata": {
        "id": "bbdnwHTiC-BJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nanogcg\n",
        "import torch\n",
        "\n",
        "from nanogcg import GCGConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.1-8B\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "for goal, target in zip(df['Goal'], df['Target']):\n",
        "    # params taken from JailbrekBench artifacts\n",
        "    config = GCGConfig(\n",
        "        num_steps=500,\n",
        "        search_width=50,\n",
        "        batch_size=512,\n",
        "        topk=256,\n",
        "        seed=42,\n",
        "        verbosity=\"WARNING\"\n",
        "    )\n",
        "\n",
        "    result = nanogcg.run(model, tokenizer, goal, target, config)\n",
        "    prompt = result.best_string\n",
        "    prompts.append(prompt)\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to('cuda')\n",
        "    outputs = model.generate(**inputs)\n",
        "    generated_tokens_ids = outputs[0]\n",
        "    answer = tokenizer.decode(generated_tokens_ids, skip_special_tokens=True)\n",
        "    answers.append(answer)"
      ],
      "metadata": {
        "id": "xmAH0yk1xku9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame.from_dict({'goal': df['Goal'], 'prompt': prompts, 'answer': answers}).to_csv(f'{model_id}_answers.csv')"
      ],
      "metadata": {
        "id": "oVBoRq4ZGy-Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}